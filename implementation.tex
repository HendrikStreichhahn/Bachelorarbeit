%!TEX root = thesis.tex

\chapter{Implementation}
\label{chapter-implementation}
	\section{Implementation Of The TADL2 Constraints}
	In this chapter, the implementation of the monitor of each constraint will be explained. This is done by giving a short documentation of each monitor is given. Additionally, the worst case memory usage and the worst case and average run time per event is shown. In section~\ref{sec:performance}, each monitor is run on traces, which were generated to match the constraints with specific parameters, to evaluate which performance can be expected in a practical usage of the implementation.\\
	All implementations have in common that they consist of 2 or 3 sections, similar to the state transition, delay (if needed) and output as defined in chapter~\ref{chapter-monitorability}. These sections are the basis for the analysis of the computational complexity, because the generated state defines the required memory capacity and the state transition function, the output function and the calculation of the required delay define the required time per timestamp with input events.\\
	The implementations are programmed and tested for version 1.2.2 of the TeSSLa interpreter.
\subsubsection{Output of the  monitors}
	The monitors are outputting RV-LTL truth values ($\top, \bot, \top^p, \bot^p$), which are represented as two boolean variables. One of these variables is showing the truth value on the prefix, which was processed until this point in time, and the other variable shows, if the output possibly changes in upcoming timestamps. These two variables are packed inside of the type \textit{fourValuedBoolean}. The individual values are mapped in the following way: 
	\begin{table}[H]
		\begin{tabular}{|c|c|c|}
			\hline
			\textit{fourValuedBoolean.value} & \textit{fourValuedBoolean.final} & RV-LTL \\
			\hline
			true & true &  $\top$\\
			\hline
			true & false &  $\top^p$\\
			\hline
			false & false & $\bot^p$ \\
			\hline
			false & true &  $\bot$\\
			\hline
		\end{tabular}
		\centering
	\end{table}
	Additionally to the state of the monitor, the previous output of the monitor is stored. If the previous output was $\bot$, the new output of the monitor is ignored and the output stays $\bot$. This is done to simplify the state and state transition of the monitor. For example in the \textit{OrderConstraint}, the number of events, which occurred in the input streams, are stored as state. In the individual input timestamps, the correct order of the events can be checked in combination of the previous output of the monitor. If the previous output is unknown, the state must be defined more complex.
	 
\subsection{DelayConstraint}
	The implementation of the \emph{DelayConstraint} monitor stores a list of $source$ events, which did not have a matching $target$ event yet as state. This list is expanded by every $source$ event, which is appended at the end of the list. If a $target$ event occurs, all matching $source$ events (possibly none) are removed from the list. Like stated in section~\ref{monitorability_DelayConstraint}, this list can grow infinitely long in worst cases, when the time domain is defined in an uncountable way. In these worst cases, an infinite number of $source$ events may occur, before any event can be removed from the list, when a matching $target$ event occurs.\\
	The used TeSSLa version is using integer values as time domain, therefore it is countable and the list cannot grow infinitely, because at most $upper$ $stimulus$ events need to be stored and the largest possible length of the list is linear dependent on the parameter $upper$. Because this list is the only growable memory usage, the algorithm is in $\mathcal{O}(upper)$ in terms of memory.\\
	In timestamps with a $target$ event, all events in the list, which are in the right time distance, are removed from the list. This means that in worst cases, all events in the list must be checked and removed, which means, the worst case run time of the state transition is linear dependent of the length of the list and therefore is $\mathcal{O}(upper)$. %In normal cases, only a few or none events must be removed, which are in the beginning of the list. Therefore, a nearly constant time behaviour can be expected.\\
	The output function checks, if the updated list of unmatched $source$ events is either empty or the event in the head of the updated list is not older than $upper$. In the first case, there is no $source$ event without matching $target$ event, therefore the output is $\top^p$. If list is not empty and the entry in the head of the list is younger than $upper$, the constraint is currently unsatisfied, but a satisfying state can still be reached. In this case, the output is $\bot^p$. If the entry in the head of the list is older than $tolerance$, there can't be a matching $target$ event, therefore the output of the monitor is $\bot$. All these checks are done in constant time, therefore the output function is in $\mathcal{O}(1)$.\\
	The required delay period is calculated by adding $upper$ to the timestamp of the head of the list of unmatched $source$ events, subtracted by the timestamp of the current event ($\mathcal{O}(1)$).
	
\subsection{StrongDelayConstraint}
	The \emph{StrongDelayConstraint} is implemented very similarly to the \emph{DelayConstraint}. The only difference in the state transition is that exactly one event, which is the head of the list of unmatched $source$ events, is removed, when a matching $target$ event occurs. Therefore, the maximal memory usage is the same ($\mathcal{O}(upper)$), but the run time of the state transition is constant per input timestamp, because only the head of the list has to be considered in the transition.\\
	The output function is nearly the same as in the previous constraint. The only difference is that in timestamps containing $target$ events, it is checked if this event has a $source$ event in the right distance. If not, the output is $\bot$. This check is also done in constant time, therefore the output function still is in $\mathcal{O}(1)$. The calculation of the delay period remains unchanged.
	
\subsection{RepeatConstraint}
	The implementation of the \emph{RepeatConstraint} stores the timestamps of the $span$ previous events as state, using TeSSLa's $last$ operator recursively (a macro called $nLastTime$ was programmed for this). Therefore, $span$ timestamps are stored and the $last$ operator is called $span$ times, which means the run time of the state transition function is linear dependent of $span$ in terms of time and the memory usage is likewise.\\
	The required delay is calculated by adding $upper$ to the  $span^{th}$ oldest event (or the first event, if there has been less than $span$ events before) minus the current timestamp. Therefore, the time for the calculation is linear dependent on the $span$ parameter, because the entire recursive definition of the state mentioned above must be walked trough.\\
	The output function checks, if the $span^{th}$ oldest event is not older than $upper$ and not younger than $lower$. If there hasn't been $span$ events before, it is checked, if the first event is not older than $upper$. If this property is fulfilled, the output is $\top^p$ and in any other case it is $\bot$. Like in the calculation of the required delay, the entire recursive definition of the considered for the evaluation. Therefore, the output function is in $\mathcal{O}(span)$.
	
\subsection{RepetitionConstraint}
	The  \emph{RepetitionConstraint} is defined as\\[10pt]
		$RepetitionConstraint(s, lower, upper, span, jitter)$\\
		$\equiv \exists X\subset \mathbb{T}: RepeatConstraint (X, lower, upper, span)$\\
		\hspace{7cm}$\land$ $StrongDelayConstraint(X, s, 0, jitter)$\\[10pt]
	The implementations of the \emph{Repeat-} and the \emph{StrongDelayConstraint} cannot be used for the implementation of this constraint, because the timestamps of $X$ are unknown.\\
	Relevant for the monitoring are the upper and lower bounds of the elements of $X$, which precede the actual events in the event stream $s$. The bounds are stored as two lists with the length of $span$. One list is containing the lower bounds for the next $span$ $X$, the other list is containing the upper bounds. At every input event, the new boundaries for the $span^{th}$ next $X$ are calculated, the lower bound by $max(List\_head(last(LowerBoundX, e)), time(e)-jitter)$ and the upper bound by $min(List\_head(last(UpperBoundX, e)), time(e))$. These new boundaries are appended to the end of the lists, while the oldest entries in the head of the lists are removed. These two lists with the size of $span$ are the only growing storage, therefore the algorithm is in $\mathcal{O}(span)$ in terms of memory. The run time of the state transition function is constant (removing the lists head and appending an entry to the lists).\\
	The output function checks, if the current timestamp is between the lower bound for the current timestamp of $X$ and $jitter$ behind the upper bound for that value. If this is the case, the output is $\top$, in any other case, it is $\bot$. Because the upper and lower bound for the current $X$ value can be directly accessed (they are the head of the lists), the output function is in $\mathcal{O}(1)$.
	
\subsection{SynchronizationConstraint}
	The \emph{SynchronizationConstraint} is defined via an application of the \emph{DelayConstraint}, but the application uses a set of unknown timestamps($\exists X: ...$), therefore the \emph{DelayConstraint} cannot be used for the implementation of this constraint.\\
	Because TeSSLa does not allow to define macros or functions with a variable number of input streams, events of each input timestamp must be placed into an integer list, which contains the index (starting at 1) of all streams, which have an event in this timestamp. This is list then used as a parameter to the implementation. The creation of this list is already implemented for up to 10 streams.\\
	The implementation of the \emph{SynchronizationConstraint} stores all events that occured not longer than $tolerance$ ago in a list. In each entry, this list contains the stream, in which the event occurred, the timestamp of the event occurrence and a boolean variable, that expresses if a fulfilled synchronization cluster for this event has already been found.\\
	This list is updated in every input timestamp in three steps. First, each event occurrences in this timestamp is appended to this list. Second, the list is separated into two parts, one with the events older and one with the events younger than $tolerance$. The part of old events is still stored in this timestamp, but removed after it. The younger events form the state that is stored for the next event occurrences. Third it is checked, if at least one event of every stream is part of the list of younger events. In this case, a fulfilled synchronization cluster has been found and the boolean variable, that states if a synchronization cluster is found for this event, is set to $true$ for all events in this list.\\
	Similar to the \emph{DelayConstraint}, this list can grow infinitely, when the time domain is uncountable, which is not the case in the used TeSSLa version. Because the TeSSLa uses integers as time domain, at most $|event|\footnote{|event| is the number of streams, not the number of events.}*tolerance$ events can occur in the $tolerance$ interval. Therefore, the algorithm is in $\mathcal{O}(|event|*tolerance)$ in terms of memory. The first step of the state transition is in $\mathcal{O}(|event|*tolerance)$, because at most $|event|$ events must be appended to the list and the list has the maximum length $tolerance$. In worst cases, every event in the list(which is in ascending order) is older than tolerance, therefore the worst case runtime of the separation in the second step of the state transition is in $\mathcal{O}(|event|*tolerance)$ in terms of time. In the third step, the complete stored list of young events must be examined, to check if the cluster is fulfilled and, if needed, every event in the list must be set to fulfilled. Therefore the third step is in $\mathcal{O}(|event|*tolerance)$ in terms of time.\\
	The output function checks first, if there are any entries in the list of stored events, which were not part of a synchronization cluster yet. If this is not the case, the output is $\top^p$, because there are no unsatisfied synchronization clusters in this case. If there are entries without a synchronization cluster so far, it is checked, if all list entries, which were removed in this timestamp (and therefore are older than $tolerance$) had a synchronization cluster. If one of these removed entries didn't have a synchronization cluster, the constraint is unsatisfied and the output is $\bot$. If all of them were part of at least one cluster, the output is $\bot^p$, because there are still entries without cluster in the list (see first check), but they still can be satisfied. Because the list can have the size $|event|*tolerance$ and all of the entries are considered in the first check of the output function, the output function is in $\mathcal{O}(|event|*tolerance)$ in terms of time.\\
	The required delay is calculated by adding $tolerance$ to the timestamp of the oldest stored unsatisfied event, subtracted by the timestamp of the current timestamp. The list is in ascending order, but the only unsatisfied events are relevant for the delay, which means, the entire list must be checked in worst cases. Therefore, the calculation of the required delay is in $\mathcal{O}(|event|*tolerance)$.
	
\subsection{StrongSynchronizationConstraint}
	The \emph{StrongSynchronizationConstraint} is defined as application of the \emph{StrongDelayConstraint}, but this application cannot be used for the implementation, like in the previous constraint. Similar to the \emph{SynchronizationConstraint}, the events of the input streams must be merged into a list.\\
	The difference between the \emph{Synchronization-} and the \emph{StrongSynchronizationConstraint} is that each event is part of exactly one synchronization cluster in the \emph{StrongSynchronizationConstraint}. Therefore, the implementation is different to the implementation of the previous constraint. Not every event is stored separately, but information about synchronization clusters, containing their start time and in which stream an event occurred in this cluster, are stored.\\
	The information about synchronization clusters are stored in a list, which contains a time expression, which marks the starting point and a map, containing a boolean variable for every input stream, which shows, if there already was an event in this stream for this cluster. At event occurrences, the event is either added to a existing synchronization cluster, or a new cluster with the start time of the event is added to the list. For the search of a matching cluster, each event of the list is considered in worst cases, therefore the run time of this part of the state transition is linear to the number of active clusters. In worst cases this number is $tolerance$, when one event occurs in every timestamp in always the same stream. In the second step of the state transition, for every stored cluster is checked, if it is fulfilled. If so, it is removed from the list. To check, if a cluster is fulfilled, one boolean check must be done for every input stream, therefore at most boolean $tolerance*|event|$ checks must be done and the worst case run time of the state transition is in $\mathcal{O}(tolerance * |event|)$. When the events occur in timewise separated synchronization clusters, the list is significantly shorter than $tolerance$ and the run time may be expected to be linear to the number of input streams.\\
	The list storing the clusters is at most $tolerance$ long and the size of individual entries of the list is linear dependent on the number of streams, because they store a boolean variable for every stream. Because of these length restrictions of the list, the algorithm is in $\mathcal{O}(|event|*tolerance)$ in terms of memory.\\
	The output function checks first, if the list of stored synchronization clusters is empty. If this is the case, the output is $\top^p$, because there are no unsatisfied synchronization clusters. If the list isn't empty, it is checked, if the oldest unsatisfied cluster , which is in the head of the list, is younger than tolerance. If so, the output is $\bot^p$, because the constraint is unsatisfied, but can be satisfied by upcoming events. If the oldest cluster is older than tolerance, the constraint is unsatisfied and cannot be satisfied by upcoming events, therefore the output us $\bot$. All these checks are done in constant time. The required delay is calculated by adding $tolerance$ to the timestamp of the oldest stored unsatisfied cluster, subtracted by the timestamp of the current timestamp ($\mathcal{O}(1)$).
	
\subsection{ExecutionTimeConstraint}
	The implementation of the \emph{ExecutionTimeConstraint} is using TeSSLa's \emph{runtime} operator on the \emph{start} and \emph{stop} events, which calculates the absolute runtime without any interruptions. The time of interruptions is also calculated by this operator and then summed up. The sum of these interruptions is reseted by every $start$ event. The calculation of this sum with resets, a macro called \textit{resetSum} was programmed, which is a modified version of TeSSLas \textit{resetCount} operator.\\
	TeSSLa's \emph{runtime} operator subtracts the timestamps of the events of the second parameter (in this case \emph{stop} and \emph{resume}) from the timestamps of the events of the first parameter(\emph{start} and \emph{preempt}), therefore it stores the timestamps of the \emph{start} and \emph{preempt} events are stored, additionally to the sum the preemptions.
	For the output, the runtime can be calculated by subtracting the second application (with $preempt$ and $resumse$ as parameters) of TeSSLa's $runtime$ operator from the sum of the first applications (with $start$ and $stop$ as parameters) of this operator. If the runtime should be checked in timestamps without a \emph{stop} event, the second parameter of the first application of the $runtime$ operator must be replaced by a current event. In the implementation this is done by merging all input streams and the delay stream together.\\
	The resulting runtime must be smaller or equal to $upper$ in any point of time and greater or equal to $lower$ at \emph{stop} events. If this is the case, the output is $\top^p$, in any other case it is $\bot$. The required delay is calculated subtracting the runtime so far from upper. All of these operations are simple arithmetic functions on timestamps, therefore the algorithm is in $\mathcal{O}(1)$ in terms of time. The required storage space is fixed, therefore it is also in $\mathcal{O}(1)$ in terms of memory.

\subsection{OrderConstraint}
	The implementation counts the number of events in the $source$ and $target$ stream and stores these numbers as state. This update is done in constant time and the required storage space is also constant. The output function compares the number of $source$ and $target$ events. If the number is equal, the constraint is fulfilled until this point in time and the output is $\top^p$. If the number $source$ events is larger, the constraint is unsatisfied, but can be satisfied by upcoming events, therefore $\bot^p$ is the output. If the number of $target$ events is larger, the order of the events is invalid, the constraint is unsatisfied and cannot be satisfied anymore. Therefore, the output is $\bot$ in these cases. The checks of the output function are also done in constant time.\\
	The introduction of new timestamps is not required for this constraint, therefore no delay period must be calculated.
	
\subsection{ComparisonConstraint}
	The \emph{ComparisonConstraint} defines comparisons between timestamps. These functionalities are already defined in TeSSLa, therefore no implementation is given as part of this thesis.  
	
\subsection{SporadicConstraint}
	The \emph{SporadicConstraint} is defined as simple application of the \emph{Repetition-} and the \emph{RepeatConstraint}, therefore the \emph{SporadicConstraint} is also implemented as application of them. The implementations of the \emph{Repetition-} and the \emph{RepeatConstraint} are both in $\mathcal{O}(span)$ in terms of time and memory. Because $span$ is fixed to 1 in the \textit{SporadicConstraint}, the implementation is in $\mathcal{O}(1)$ in terms of memory and time.
	
\subsection{PeriodicConstraint}
	The \emph{PeriodicConstraint} is defined as application of the \emph{SporadicConstraint} and is also implemented like this. Because the \emph{SporadicConstraint} is in $\mathcal{O}(1)$ in terms of memory and time, the \emph{PeriodicConstraint} is also.
	
\subsection{PatternConstraint}
	The \emph{PatternConstraint} is defined as application of the \emph{Periodic-}, \emph{Delay-} and \emph{RepeatConstraint}. Because of the set of unknown timestamps $X$, the \emph{Periodic-} and \emph{DelayConstraint} cannot be used for the implementation. The set $X$ is not used in the application of the \emph{RepeatConstraint}, therefore its implementation is used as part of the output function.\\
	The implementation of the \emph{RepeatConstraint} is in $\mathcal{O}(span)$ in terms of time memory. The $span$ attribute is set to 1 in the application, therefore the run time and memory usage is constant in this part.\\
	In the implementation of the \emph{PatternConstraint}, the lower and upper bound for the current timestamp of $X$ is stored. At every event, these bounds are further enclosed, taking the previous known bounds and the bounds implied by the current event
	\begin{align}
		x\in X: &time(event)-\text{\emph{offset}}_{count(event)\text{ mod }|\text{\emph{offset}}|}-jitter \leq x\\
			     &\leq  time(event)-\text{\emph{offset}}_{count(event)\text{ mod }|\text{\emph{offset}}|}
	\end{align}
	into account. The new lower bound is set by using the maximum of the previous lower bound and the lower bound implied by the current event, the new upper bound by using the minimum of the previous upper bound and the upper bound implied by the current event. At every $|\text{\emph{offset}}|^{th}$ event, $period$ is added to the current bounds. The access of the map entries is done in constant time, therefore the calculation of these new borders is also done in constant time and the state transition function in $\mathcal{O}(1)$ in terms of time.\\
	The output function checks, if the timestamp of the current event is between the lower bound plus $\text{\emph{offset}}_{count(event)\text{ mod }|\text{\emph{offset}}|}$ and the upper bound plus \\$\text{\emph{offset}}_{count(event)\text{ mod }|\text{\emph{offset}}|}$ plus $jitter$. If so, the output is $\top^p$, if not it is $\bot$. The previous defined output is conjuncted with the output of the application of the \textit{RepeatConstraint}. The comparisons of timestamps are done in constant time and monitoring the \textit{RepeatConstraint} with $span=1$ if likewise. Therefore, the output function is in $\mathcal{O}(1)$.\\
	The required delay is defined by the time distance between the current timestamp and the upper bound for X, plus the expected offset of the following event, plus the allowed deviation ($jitter$).\\
	The only state stored in the implementation are the upper and lower bound for the  current $x$-value, therefore the implementation itself is in $\mathcal{O}(1)$ in terms of memory, but the size of the \textit{offset}-parameter, which is a map, is not limited in size and the complete algorithm, including the parameters, is $\mathcal{O}(|\text{\textit{offset}}|)$ in terms of memory.
	
\subsection{ArbitraryConstraint}
	The \emph{ArbitraryConstraint} is defined as multiple applications of the \emph{RepeatConstraint} and is also implemented this way. The number of applications of the \emph{RepeatConstraint} is dependent on the number of elements in the $minimum$ and $maximum$ parameters. The runtime of the \emph{RepeatConstraint} is in $\mathcal{O}(1)$ per application and event, therefore it the \emph{ArbitraryConstraint} is in $\mathcal{O}(|minimum|)$ in terms of time. The memory usage of the \emph{RepeatConstraint} is in $\mathcal{O}(span)$. In the application of the \emph{RepeatConstraint}, the $span$ parameter increases for each of the $|minimum| = |maximum|$ applications. Therefore, the implementation is in $\mathcal{O}(\sum_{i=1}^{|minimum|}i)\widehat{=}\mathcal{O}(|minimum|^2+|minimum|)$ in terms of time.

\subsection{BurstConstraint}
	The \textit{BurstConstraint} is defined as twofold application of the \emph{RepeatConstraint} and is also implemented this way. The \textit{RepeatConstraint} is in $\mathcal{O}(span)$ in terms of time and memory. Because the $span$ attribute is set to 1 and $maxOccurrences$ in the applications of the \textit{RepeatConstraint}, the implementation of the \emph{BurstConstraint} is in $\mathcal{O}(maxOccurrences)$ in terms of memory and time.

\subsection{ReactionConstraint}
	The correctness of the \textit{EventChain} is assumed in the implementation. If this property is unknown, it must be checked individually.\\
	The implementation of the \emph{ReactionCostraint} stores a map, which maps the color of $stimulus$ events, which did not have a matching $response$ event yet, to their timestamps. This state is updated at every input event. $Stimulus$ events are inserted into the map, $response$ events remove, if possible, a event from the map called above. Similar to the \emph{DelayConstraint}(the \emph{ReactionCostraint} can be seen an extension of the \emph{DelayConstraint}, that additionally considers the color of events), the maximal number of entries in the map is the maximal number of $stimulus$ events, that could possibly occur in an interval of the length $maximum$, which is $maximum$.  Therefore, the algorithm is in $\mathcal{O}(maximum)$ in terms of memory. The state transition (insertion, lookup and possibly remove in map) is in $\mathcal{O}(1)$ in terms of time.\\
	The required delay is calculated by adding $maximum$ to the timestamp of the oldest entry in the map mentioned above and subtracting the current timestamp. Because the map is unsorted, every entry of the map must be considered for this. Therefore, the calculation of the required delay is in the time complexity class $\mathcal{O}(maximum)$.\\
	The output function first checks, if the map of unmatched $stimulus$ events is empty. If so, the constraint is satisfied and the output is $\top^p$. If there are entries in the map and the oldest entry is older than $tolerance$, the constraint is unsatisfied and cannot be satisfied by upcoming events. In this case, the output is $\bot$. If the oldest entry is younger than $tolerance$, the constraint is currently unsatisfied, but can be satisfied by upcoming events. Therefore, the output is $\bot^p$. To find the oldest entry in the map, all entries must be considered. Therefore, the output function is linear dependent on the size of this map, which is at most $maximum$.
	
\subsection{AgeConstraint}
	Like before, the correctness of the \textit{EventChain} is assumed in the implementation. If this property is unknown, it must be checked individually.\\
	Similar to the implementation of the \emph{ReactionCostraint}, the \emph{AgeConstraint} monitor stores a map containing the latest $stimulus$ event, which are younger than \textit{maximum}. The $color$ value is used as map key and the timestamp is used as map value. This map has the maximal size $maximum$ and is updated at every input event. $Stimulus$ events are inserted or updated, and entries, that are older than $maximum$ are removed. To make this update faster, a list containing the colors of the events in the map is stored additionally. The maximal size of this list is also $maximum$ and the colors are stored in chronological order, so that the color, that occurred the longest time ago, is in the head of the list. The update is done by looking at the head of the list and removing this entry from the list and the corresponding entry with the same color from the map, if the entry is older than $maximum$. These operations are done in constant time, but need to be repeated, as long as the color in the head of the map is too old, so at most $maximum$ times. Inserting or updating the $stimulus$ event to the map is done in effectively constant time, but inserting or updating the list requires to remove any previous entry with the color of the current event. For this, every entry in the map has to be processed, which means this operation takes $maximum$ steps in worst cases. Consecutively, the state and the state transition is in $\mathcal{O}(maximum)$ in terms of memory and time. The creation of new timestamps is not needed in this constraint, because only previous events need to be considered, upcoming events not.\\
	In timestamps containing a $response$ event, the output function checks, if a $stimulus$ event with the same color is in the map and if the time distance between them is greater or equal to $minimum$ and smaller or equal to $maximum$. if so, the output is $\top^p$, if not, it is $\bot$. Timestamps without $response$ events cannot lead to a violation of the constraint. The lookup in the map and the comparisons are done in constant time.

\subsection{OutputSynchronizationConstraint}
	Similar to the \emph{Synchronization-} and \emph{StrongSynchronizationConstraint}, the input streams cannot be directly used as parameter. For the \emph{OutputSynchronizationConstraint}, a stream of maps must be created, which represents the events of each timestamp. The key of each entry is the index of the stream (0 for the $stimulus$ stream, 1, 2, ... for the \textit{response} streams), in which the event occurred and the value is the color of the event. Again, the creation of this map is already implemented for up to 10 $response$ streams.\\
	In the \emph{OutputSynchronizationConstraint}, for each $stimulus$ event, there must be one synchronization cluster of the length $tolerance$, in which each $response$ stream must have at least one event of the same color as the $stimulus$ event. There is no time distance between this cluster and the $stimlus$ event defined, it just has to be before the end of the streams.\\
	The implementation of the \emph{OutputSynchronizationConstraint} is storing three different information as state. First, a set of the stimulus colors, which did not have a $response$ event in the same color yet. This set is updated at every input event, the color of $stimulus$ events is inserted and the colors of the $response$ events in the current timestamp are removed from the set. These updates are done in constant time, in worst cases, where no matching $response$ events are occurring, the required storage space is linear dependent on the number of $stimulus$ events.\\
	The second information is a map, which is containing information about all synchronization clusters that were not finished before this point in time. This map is using the color attribute as key and the start timestamp and a map as value. This inner map uses the indices of the \textit{response} streams as keys and a boolean variable as value. This value shows, whether there was an event for this synchronization cluster in this stream or not. This map is updated at every $response$ event. For each of these $response$ events, it is checked, if a synchronization cluster with a matching color exists, if not, a new synchronization cluster with the color of the event is created, if the color of this event was in the set of stimulus colors of the previous timestamp. The check per event (two lookups in maps, one on set) is done in constant time, therefore the entire update of this map is in $\mathcal{O}(|response|)$ in terms of time per input timestamp. In worst cases, each event results in the creation of a new synchronization cluster, which must be stored at least for the length of $tolerance$. The size of each information about one synchronization cluster is linear dependent on the number of $response$ streams and in each interval of the length $tolerance$, $tolerance*|response|$ events can occur and create a new synchronization cluster, therefore this information is in $\mathcal{O}(tolerance*|response|^2)$ in terms of memory.\\
	The third stored information is similar to the second, but the clusters that are either older than tolerance or fulfilled are removed from the map. Therefore, the worst case memory consumption is the also $\mathcal{O}(tolerance*|response|^2)$. To remove fulfilled clusters, it is checked for each cluster in the map, if there was at least one event in each $response$ stream of the color of the cluster. Therefore, this update is in $\mathcal{O}(tolerance*|response|^2)$ in terms of time.\\
	In combination, the run time of the state transition is in $\mathcal{O}(tolerance*|response|^2)$ and the memory usage is in $\mathcal O(count(stimulus) + tolerance*|response|^2)$.\\
	The required delay is calculated by adding $tolerance$ to the start time of the oldest unfinished cluster and subtracting the current timestamp. To get the oldest unfinished synchronization cluster, all map currently active clusters must be considered, which means the run time is linear dependent on the number number currently active clusters (at most $tolerance*|response|$).\\
	The output function checks first, if the set of unmatched \textit{stimulus} and the list of map of stored stored synchronization clusters are empty. If this is the case, not \textit{stimulus} events had a fulfilled synchronization and the constraint is fulfilled until this point in time. The output is $\top^p$. If the set or the map are not empty it is checked, if the all synchronization clusters are younger than $tolerance$. If so, the constraint is currently unsatisfied, but can be satisfied by future events. In this case, the output is $\bot^p$. If the oldest synchronization cluster is older than $tolerance$, the constraint is unsatisfied and no future events can't change this. Therefore, the output is $\bot$ in this case. The first check is done in constant time, the second check requires to consider each stored synchronization cluster. Therefore, the run time of the output is linear dependent on the number of stored events, which is at most $tolerance*|response|$.
	
%	The implementation of the \emph{OutputSynchronizationConstraint} is storing four different informations as state. First, a list of every color that occurred in $stimulus$. This is updated at every $stimulus$ event by appending its color to the list(run time: $\mathcal{O}(1)$, memory: $\mathcal{O}(count(stimulus))$).\\
%	Second, a map is stored, which is containing information about all synchronization clusters that were not finished before this point in time. This map is using the color attribute as key and the start timestamp and a map as value. This inner map uses the indices of the \textit{response} streams as keys and a boolean variable as value. This value shows, whether there was an event for this synchronization cluster in this stream or not. This map is updated at every $response$ event. For each of these $response$ events, it is checked, if a synchronization cluster with a matching color exists, if not, a new synchronization cluster with the color of the event is created. The check per event (two lookups in maps)  is done in constant time, therefore the entire update of this map is in $\mathcal{O}(|response|)$ in terms of time per input timestamp. In worst cases, each event results in the creation of a new synchronization cluster, which must be stored at least for the length of $tolerance$. The size of each information about one synchronization cluster is linear dependent on the number of $response$ streams and in each interval of the length $tolerance$, $tolerance*|response|$ events can occur and create a new synchronization cluster, therefore this information is in $\mathcal{O}(tolerance*|response|^2)$ in terms of memory. 
%	The third stored information is similar to the second, but the clusters that are either older than tolerance or fulfilled are removed from the map. Therefore, the worst case memory consumption is the also $\mathcal{O}(tolerance*|response|^2)$. To remove fulfilled clusters, it is checked for each cluster in the map, if there was at least one event in each $response$ stream of the color of the cluster. Therefore, this update is in $\mathcal{O}(tolerance*|response|^2)$ in terms of time.
%	The fourth stored information is a set of all colors that had a fulfilled synchronization cluster in the $response$ streams until this point in time. Inserting items into the set is done in constant time. The number of fulfilled synchronization clusters is at most the number events in all $response$ streams, divided by the number of the $response$ streams. Therefore, the required memory of this information is in $\mathcal{O}\left(\frac{\sum_i count(response_i)}{|response|}\right)$.\\
%	The combined time complexity class is $\mathcal{O}(tolerance*|response|^2)$. The combined memory complexity classes, which defines the memory complexity of the algorithm, is $\mathcal{O}\left(count(stimulus)+\frac{\sum_i count(response_i)}{|response|}+tolerance*|response|^2\right)$.\\
%	The required delay is calculated by adding $tolerance$ to the start time of the oldest unfinished cluster and subtracting the current timestamp ($\mathcal{O}(tolerance*|response|^2)$).\\
%	The output function checks that all stored synchronization clusters are either younger than $tolerance$ or fulfilled.
%	Because the entries of the map, that stores the synchronization clusters, cannot be accessed in way, that is sorted by age, every entry of the map must be checked for its age (at most $tolerance*|response|$ checks). For every synchronization cluster that is older than $tolerance$, it must be checked, if this cluster is fulfilled. The check of a single cluster requires to check the boolean variables of each stream. Per timestamp, at most $|response|$ synchronization clusters can be started, therefore at most $response$ clusters grow older than $tolerance$ per timestamp. Therefore, the output function is in $\mathcal{O}(tolerance*|response|^2)$ in terms of time per input timestamp.\\
%	At the end of the observation, it must be checked, if each $stimulus$ event had a matching synchronization cluster. For each of the at most $count(stimulus)$ $stimulus$ colors, a lookup in a set must be done, therefore this check is in $\mathcal{O}(count(stimulus))$ and the complete output function, including the check at the end of observation, is in $\mathcal{O}(tolerance*|response|^2 + count(stimulus))$ in terms of time.



\subsection{InputSynchronizationConstraint}
	The input streams must be transformed into a $map[Int, Int]$ stream, similar to the previous constraint, but this time the index 0 indicates the $response$ stream and the indices 1, 2, ... are indicating the $stimulus$ streams.\\
	The \emph{InputSynchronizationConstraint} is defined very similar to the \emph{OutputSynchronizationConstraint}. The difference is, that the synchronization occurs in a set of $stimulus$ events, not in $response$ events.\\
	Despite the similarities, the implementation of the \emph{InputSynchronizationConstraint}  is different to the implementation of the \emph{OutputSynchronizationConstraint}. As state, a map that uses the numbers 1 to $|stimulus|$ as keys and as values a second map that uses colors (integer) as key and the timestamp of the latest occurrence of this color in the stream as value. This map is updated at every $stimulus$ event, at which either the timestamp of the latest occurrence of this color in this stream is updated, or a new inner map entry is created for this color.  The lookup, if there already is a matching entry in the map for this color in this stream and possibly its update is done in constant time, but the time for initializing a new entry is linear dependent on the number of $stimulus$ streams. Because $|stimulus|$ events may occur and introduce a new color in each timestamp, the state transition is in $\mathcal{O}(|stimulus|^2)$ in terms of time. The worst case memory size of this information is in $\mathcal{O}(|stimulus|*count(stimulus))$, because the map described above possibly stores every input event of the $stimulus$ streams, when they introduce a new color and therefore a new entry in the inner map of the stream must be created. $Response$ events are not considered for the state of the monitor.\\
	The creation of new timestamps is not needed in this constraint, because only previous events need to be considered. Therefore, the calculation of a delay span is not required.\\
	In timestamps containing a $response$ event the output function checks,if the last occurrences of the corresponding color in the $stimulus$ stream form a valid synchronization cluster. This is done by searching the youngest and oldest event with this color in the map of latest $stimulus$ events. If a event of this color is missing, the age is interpreted as $\infty$ or $-\infty$, which leads to a length of the synchronization cluster that is definitely longer than $tolerance$. If the synchronization cluster is longer than $tolerance$, the constraint is violated and the output is $\bot$. If the cluster is not longer than $tolerance$, the output is $\top^p$. In timestamps without a $response$, the output remains unchanged. Because the color value is the key of the inner map, the time for searching the oldest and youngest event of this color is linear to the number of $stimulus$ streams. Therefore, the output function is in $\mathcal{O}(|stimulus|)$ in terms of time.
	
\subsection{EventChain}
	Additionally to the 18 TADL2 timing constraints, a monitor, which checks the correctness of \textit{EventChains} was implemented. A \textit{EventChain} is defined on a $stimulus$ and a $response$ stream as:\\[10pt]
	$\forall x \in stimulus:\forall y\in response: x.color=y.color\Rightarrow x<y$\\[10pt]
	As a state, a set, which contains all colors that previously occurred in $reponse$ is stored. This set is updated at each $response$  event by a an insertion into a set ($\mathcal{O}(1)$). The maximal size of this map is the number of events in $response$, therefore the state is in $\mathcal{O}(count(response))$ in terms of memory.\\
	The output function checks, if the color every occurring $stimulus$ event is not in the set of $response$ colors, which is checked in constant time. If the color is in the set of $response$ colors, the output is $\bot$. Otherwise, it is $\top^p$.
	
\subsection{Conclusion}
Table~\ref{tab:complexityClasses} gives an overview of the worst case memory consumption and the worst case run time per input timestamp. The worst case memory requirement and the runtime per input timestamp of the \textit{Repeat-}, \textit{Repetition-}, \textit{ExecutionTime-}, \textit{Sporadic-}, \textit{Periodic-}, \textit{Pattern-}, \textit{Arbitrary-} and \textit{BurstConstraint}, which are the \textit{simple monitorable} constraints, are either constant, or they are only limited by the parameters of the constraint, not by the input traces. The implementations of the \textit{Delay-}, \textit{StrongDelay-}, \textit{Synchronization-}, \textit{StrongSynchronization-}, \textit{Reaction-} and \textit{AgeConstraint} are limited by the events, which may occur in time intervals of a specific length. Monitoring the correctness of \textit{EventChains}, the \textit{OutputSynchronization-} or the \textit{InputSynchronizationConstraint} with these implementations require continuously growing memory resources and in the \textit{OutputSynchronizationConstraint}, the run time per input timestamp is continuously growing too. The implementation of the \textit{OrderConstraint} is in $\mathcal{O}(1)$ in terms of memory and time per event, although it is classified as \textit{Not simple monitorable}. This is, because integers of a fixed length are used for the implementation of the constraint and only a finite subset of all streams that fulfill the constraint can be monitored correctly.

	\begin{table}
		\begin{tabular}{|c|c|c|}
			\hline
			& Memory & \makecell{Run Time per Input\\Timestamp} \\
			\hline
			{DelayConstraint} & $\mathcal{O}(upper)$ & $\mathcal{O}(upper)$ \\
			\hline
			{StrongDelayConstraint} &  $\mathcal{O}(upper)$ &  $\mathcal{O}(1)$ \\
			\hline
			{RepeatConstraint} & $\mathcal{O}(span)$ & $\mathcal{O}(span)$ \\
			\hline
			{RepetitionConstraint} & $\mathcal{O}(span)$ & $\mathcal{O}(1)$ \\
			\hline
			SynchronizationConstraint & $\mathcal{O}(|event|*tolerance)$ & $\mathcal{O}(|event|*tolerance)$ \\
			\hline
			StrongSynchronizationConstraint & $\mathcal{O}(|event|*tolerance)$ & $\mathcal{O}(|event|*tolerance)$ \\
			\hline
			ExecutionTimeConstraint & $\mathcal{O}(1)$ & $\mathcal{O}(1)$ \\
			\hline
			OrderConstraint & $\mathcal{O}(1)$ & $\mathcal{O}(1)$ \\
			\hline
			SporadicConstraint & $\mathcal{O}(1)$ & $\mathcal{O}(1)$ \\
			\hline
			PeriodicConstraint & $\mathcal{O}(1)$ & $\mathcal{O}(1)$ \\
			\hline
			PatternConstraint & $\mathcal{O}(1)$ & $\mathcal{O}(1)$ \\
			\hline
			ArbitraryConstraint & $\mathcal{O}(|minimum|)$& \makecell{$\mathcal{O}(|minimum|^2$\\$+|minimum|)$} \\
			\hline
			BurstConstraint & $\mathcal{O}(maxOccurrences)$ & $\mathcal{O}(maxOccurrences)$ \\
			\hline
			ReactionConstraint & $\mathcal{O}(maximum)$ & $\mathcal{O}(maximum)$ \\
			\hline
			AgeConstraint & $\mathcal{O}(maximum)$ & $\mathcal{O}(maximum)$ \\
			\hline
			OutputSynchronizationConstraint& \makecell{$\mathcal{O}(count(stimulus)$\\$+tolerance*|response|^2)$} &  \makecell{$\mathcal{O}(tolerance$\\$*|response|^2)$}\\
			\hline
			InputSynchronizationConstraint& \makecell{$\mathcal{O}(|stimulus|$\\$*count(stimulus))$} & $\mathcal{O}(|stimulus|^2)$ \\
			\hline
			EventChain & $\mathcal{O}(count(response))$ & $\mathcal{O}(1)$\\
			\hline
		\end{tabular}
		\centering
		\label{tab:complexityClasses}
		\caption{Worst Case Run Times of the Implementations}
	\end{table}

	
	
